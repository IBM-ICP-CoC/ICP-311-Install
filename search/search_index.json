{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Installing IBM Cloud Private in IBM Cloud Virtual Machines This document will walk you through the steps needed to create virtual machines in IBM Cloud and use them to run IBM Cloud Private. It should be noted that the instructions on this site were captured while configuring and installing on RHEL virtual servers hosted on IBM Cloud (Softlayer). Some of the server setup steps may vary if you are attempting to provision virtual servers in another cloud provider or if you are installing on a different OS. Tip The latest version of this document can always be found at https://ibm.biz/installicp . If you find any typos, errors, or just want to provide helpful feedback to make this document better, please click the Github link at the bottom of the left navigation menu and create an issue in the repository. Thanks for your feedback! In this example, we will be installing Docker EE and ICP 3.1.1 Enterprise edtion on RHEL. The docker EE edition is install file is included with the download of ICP 3.1.1. These are the files you will need. ibm-cloud-private-x86_64-3.1.1.tar.gz icp-docker-18.03.1_x86_64.bin Note If you are an IBM'er, you can download the install files from the Software Sellers Workplace . You will need to log in and do a search for IBM Cloud Private 3.1.1 . The package assembly you need to look for is: IBM Cloud Private Installation Packages eAssembly (CJ4MTEN) IBM Cloud Private 3.1.1 for Linux (x86_64) Docker (CNZ4WEN) IBM Cloud Private 3.1.1 Docker for Linux (x86_64) (CNXD2EN) Acknowledgments Dave Wakeman (Public) & Dave Krier (Distribution) are the authors of this install guide. Dave Weilert (COC Team) was the pioneer who documented the install process and perfected all the pre-requisite steps. Jim Conallen (COC Team) authored the LDAP setup and configuration guide. Tom Watson (Distribution) tested and added comments. Danger This is NOT a replacement for the official documentation for IBM Cloud Private! It is intended only to be a learning guide, and uses an arbitrary configuration that may not be appropriate for production use.","title":"Home"},{"location":"#installing-ibm-cloud-private-in-ibm-cloud-virtual-machines","text":"This document will walk you through the steps needed to create virtual machines in IBM Cloud and use them to run IBM Cloud Private. It should be noted that the instructions on this site were captured while configuring and installing on RHEL virtual servers hosted on IBM Cloud (Softlayer). Some of the server setup steps may vary if you are attempting to provision virtual servers in another cloud provider or if you are installing on a different OS. Tip The latest version of this document can always be found at https://ibm.biz/installicp . If you find any typos, errors, or just want to provide helpful feedback to make this document better, please click the Github link at the bottom of the left navigation menu and create an issue in the repository. Thanks for your feedback! In this example, we will be installing Docker EE and ICP 3.1.1 Enterprise edtion on RHEL. The docker EE edition is install file is included with the download of ICP 3.1.1. These are the files you will need. ibm-cloud-private-x86_64-3.1.1.tar.gz icp-docker-18.03.1_x86_64.bin Note If you are an IBM'er, you can download the install files from the Software Sellers Workplace . You will need to log in and do a search for IBM Cloud Private 3.1.1 . The package assembly you need to look for is: IBM Cloud Private Installation Packages eAssembly (CJ4MTEN) IBM Cloud Private 3.1.1 for Linux (x86_64) Docker (CNZ4WEN) IBM Cloud Private 3.1.1 Docker for Linux (x86_64) (CNXD2EN)","title":"Installing IBM Cloud Private in IBM Cloud Virtual Machines"},{"location":"#acknowledgments","text":"Dave Wakeman (Public) & Dave Krier (Distribution) are the authors of this install guide. Dave Weilert (COC Team) was the pioneer who documented the install process and perfected all the pre-requisite steps. Jim Conallen (COC Team) authored the LDAP setup and configuration guide. Tom Watson (Distribution) tested and added comments. Danger This is NOT a replacement for the official documentation for IBM Cloud Private! It is intended only to be a learning guide, and uses an arbitrary configuration that may not be appropriate for production use.","title":"Acknowledgments"},{"location":"addnfs/","text":"Add NFS Storage to your cluster Create the File Storage Device in IBM Cloud IBM Cloud has a File Storage service that you can use to create NFS volumes that can be added as persistent volumes in your ICP cluster. To provision an NFS volume login to IBM Cloud and follow the steps below. Note These instructions have only been tested with an IBM Cloud Private cluster hosted in virtual servers in IBM Cloud. Go to the catalog, navigate to the Storage category and click on the File Storage tile. Click the Create button. Select the appropriate region and (optionally) data center. Note In this example the default performance option is selected. You can choose whatever you need. Click the check box to accept the terms and conditions, then click Create . You should see a popup indicating that your order was placed. Click the button to view the dashboard. The dashboard does not show file storage resources, so you will need to click the file storage link to see them. You should see a list of all your file storage devices. If you have more than one the one you just created should be at the bottom of the list. Note If your device has not yet been provisioned you will see a little \"clock\" icon next to the name of the device. In this case just refresh the page using the refresh icon in the table's toolbar until the clock icon goes away. Click on the name of your device to see its details. Take note of the Host Name and Mount Point fields. You will need them later to configure your persistent volume in Cloud Private. In order for Cloud Private to be able to access your newly created storage you must authorize the hosts on which your cluster is running. If you scroll down on the device details page you will notice that by default no hosts are authorized. Click the Authorize Host link. In the popup set the Device Type to Virtual Server (assuming you used virtual servers), then use the Virtual Guest dropdown to select each of your Cloud Private servers (one at a time). Click Submit . You should now see all of your virtual servers listed in the Authorized Hosts table on the Device Details page. This concludes the portion of the setup in IBM Cloud. Add Persistent Volume in IBM Cloud Private Login to your Cloud Private Console. In the left navigation menu expand the Platform category and click Storage . Click Create Persistent Volume . On the General tab give your volume a name. Leave the Storage class name blank, set the Capacity field to the size of your file storage volume (20 GB by default, or whatever size you chose when you created the volume). Select the appropriate Access mode that you wish to use for this volume. Click the Parameters tab. For an NFS volume you have to add two parameters, named Server and Path , whose values are the host name and mount point you captured earlier on the Device Details page for your volume in IBM Cloud. Warning The Mount Point field will actually be of format hostname:path , like this: fsf-dal1301d-fz.adn.networklayer.com:/IBM02SEV1578085_10/data01 The value of path should be just the content after the : , like this: /IBM02SEV1578085_10/data01 Click Create . You should see your newly created persistent volume in the list. That's it! If the status is Available your persistent volume is ready for use.","title":"Adding NFS Storage"},{"location":"addnfs/#add-nfs-storage-to-your-cluster","text":"","title":"Add NFS Storage to your cluster"},{"location":"addnfs/#create-the-file-storage-device-in-ibm-cloud","text":"IBM Cloud has a File Storage service that you can use to create NFS volumes that can be added as persistent volumes in your ICP cluster. To provision an NFS volume login to IBM Cloud and follow the steps below. Note These instructions have only been tested with an IBM Cloud Private cluster hosted in virtual servers in IBM Cloud. Go to the catalog, navigate to the Storage category and click on the File Storage tile. Click the Create button. Select the appropriate region and (optionally) data center. Note In this example the default performance option is selected. You can choose whatever you need. Click the check box to accept the terms and conditions, then click Create . You should see a popup indicating that your order was placed. Click the button to view the dashboard. The dashboard does not show file storage resources, so you will need to click the file storage link to see them. You should see a list of all your file storage devices. If you have more than one the one you just created should be at the bottom of the list. Note If your device has not yet been provisioned you will see a little \"clock\" icon next to the name of the device. In this case just refresh the page using the refresh icon in the table's toolbar until the clock icon goes away. Click on the name of your device to see its details. Take note of the Host Name and Mount Point fields. You will need them later to configure your persistent volume in Cloud Private. In order for Cloud Private to be able to access your newly created storage you must authorize the hosts on which your cluster is running. If you scroll down on the device details page you will notice that by default no hosts are authorized. Click the Authorize Host link. In the popup set the Device Type to Virtual Server (assuming you used virtual servers), then use the Virtual Guest dropdown to select each of your Cloud Private servers (one at a time). Click Submit . You should now see all of your virtual servers listed in the Authorized Hosts table on the Device Details page. This concludes the portion of the setup in IBM Cloud.","title":"Create the File Storage Device in IBM Cloud"},{"location":"addnfs/#add-persistent-volume-in-ibm-cloud-private","text":"Login to your Cloud Private Console. In the left navigation menu expand the Platform category and click Storage . Click Create Persistent Volume . On the General tab give your volume a name. Leave the Storage class name blank, set the Capacity field to the size of your file storage volume (20 GB by default, or whatever size you chose when you created the volume). Select the appropriate Access mode that you wish to use for this volume. Click the Parameters tab. For an NFS volume you have to add two parameters, named Server and Path , whose values are the host name and mount point you captured earlier on the Device Details page for your volume in IBM Cloud. Warning The Mount Point field will actually be of format hostname:path , like this: fsf-dal1301d-fz.adn.networklayer.com:/IBM02SEV1578085_10/data01 The value of path should be just the content after the : , like this: /IBM02SEV1578085_10/data01 Click Create . You should see your newly created persistent volume in the list. That's it! If the status is Available your persistent volume is ready for use.","title":"Add Persistent Volume in IBM Cloud Private"},{"location":"createvms/","text":"Create the Virtual Machines Provision 5 virtual machines with this configuration: RHEL Minimal (7.x) 8 CPU / 16G Mem 100GB Boot Disk 200GB Disk 1 500GB Disk 2 Tip You can create all 5 VMs at the same time with the same configuration by using the Quantity field on the order page. To create virtual machines: Login to your IBM Cloud account. Click the Create Resource button on the Dashboard page Click the Virtual Server tile. Specify the following settings for your Virtual Server Instance: The Public Virtual Server tile should be selected; click Continue . Choose Public as the type of virtual server Set the Quantity to 5 Specify values for Host Name and Domain Choose the location and data center of your choice Choose the Balanced B1.8x16 profile. You many need to choose the All Profiles tab to find it. Choose Red Hat as the Image (take the default 7.x Minimal (64-bit) - HVM ). Make sure you actually cick the Red Hat box and that it has a blue outline and the blue checkmark. In the Attached Storage Disks section, click the Add New button twice to create two new disks, named Disk 1 and Disk 2 change the size of the disks: Boot Disk to 100 GB Disk 1 to 200 GB Disk 2 to 500 GB When finished the screen should look like this: Click the checkbox to acknowledge that you have read and agree to the third party agreements and click Create . Note These values for host name and domain do not have any affect on the actual host name and domain defined inside the virtual machines. They are just labels and can be changed at any time. Note When you chose a quantity greater than 1 your virtual machines may have an added sequence number in their host names. Don't forget that you can change the names of your virtual machines after they are created. The name of your VM's in the IBM Cloud Infrastructure section are just for your own reference. It's best practice to name each of them with some indication on what role they will play in your cluster, such as (master, mgmt, worker, va, proxy) Tip You will need to transfer a very large file to your master node. Once your machines are created, you can initiate this transfer. You will need to know the password for root , which you can get from the Passwords tab on the Device Details page. To transfer the file open a terminal window on your host machine, navigate to the directory where the file is stored and execute this command: scp ibm-cloud-private-x86_64-3.1.1.tar.gz root@<your Master node IP>:/tmp The scp command is the Secure File Transfer command. When your machines are provisioned it will be helpful for you to collect some information together that you will need later. The table below is an example of what you should collect. The hostnames can be whatever values you want. The passwords are for the root user and can be obtained on the Passwords tab of the Details page for your device. Machine Role hostname IP Address Password Master/Proxy/Boot my-icp-master xxx.xxx.xxx.xxx xxxxxxxx Management my-icp-mgmt xxx.xxx.xxx.xxx xxxxxxxx VA my-icp-va xxx.xxx.xxx.xxx xxxxxxxx Worker1 my-icp-worker1 xxx.xxx.xxx.xxx xxxxxxxx Worker2 my-icp-worker2 xxx.xxx.xxx.xxx xxxxxxxx Note I recommend you use the Public IPs for all your virtual machines. You can use the Private IP for everything except the PROXY node if you really want to, but you must be consistent in all the next steps for everything to work properly.","title":"Creating VMs"},{"location":"createvms/#create-the-virtual-machines","text":"Provision 5 virtual machines with this configuration: RHEL Minimal (7.x) 8 CPU / 16G Mem 100GB Boot Disk 200GB Disk 1 500GB Disk 2 Tip You can create all 5 VMs at the same time with the same configuration by using the Quantity field on the order page. To create virtual machines: Login to your IBM Cloud account. Click the Create Resource button on the Dashboard page Click the Virtual Server tile. Specify the following settings for your Virtual Server Instance: The Public Virtual Server tile should be selected; click Continue . Choose Public as the type of virtual server Set the Quantity to 5 Specify values for Host Name and Domain Choose the location and data center of your choice Choose the Balanced B1.8x16 profile. You many need to choose the All Profiles tab to find it. Choose Red Hat as the Image (take the default 7.x Minimal (64-bit) - HVM ). Make sure you actually cick the Red Hat box and that it has a blue outline and the blue checkmark. In the Attached Storage Disks section, click the Add New button twice to create two new disks, named Disk 1 and Disk 2 change the size of the disks: Boot Disk to 100 GB Disk 1 to 200 GB Disk 2 to 500 GB When finished the screen should look like this: Click the checkbox to acknowledge that you have read and agree to the third party agreements and click Create . Note These values for host name and domain do not have any affect on the actual host name and domain defined inside the virtual machines. They are just labels and can be changed at any time. Note When you chose a quantity greater than 1 your virtual machines may have an added sequence number in their host names. Don't forget that you can change the names of your virtual machines after they are created. The name of your VM's in the IBM Cloud Infrastructure section are just for your own reference. It's best practice to name each of them with some indication on what role they will play in your cluster, such as (master, mgmt, worker, va, proxy) Tip You will need to transfer a very large file to your master node. Once your machines are created, you can initiate this transfer. You will need to know the password for root , which you can get from the Passwords tab on the Device Details page. To transfer the file open a terminal window on your host machine, navigate to the directory where the file is stored and execute this command: scp ibm-cloud-private-x86_64-3.1.1.tar.gz root@<your Master node IP>:/tmp The scp command is the Secure File Transfer command. When your machines are provisioned it will be helpful for you to collect some information together that you will need later. The table below is an example of what you should collect. The hostnames can be whatever values you want. The passwords are for the root user and can be obtained on the Passwords tab of the Details page for your device. Machine Role hostname IP Address Password Master/Proxy/Boot my-icp-master xxx.xxx.xxx.xxx xxxxxxxx Management my-icp-mgmt xxx.xxx.xxx.xxx xxxxxxxx VA my-icp-va xxx.xxx.xxx.xxx xxxxxxxx Worker1 my-icp-worker1 xxx.xxx.xxx.xxx xxxxxxxx Worker2 my-icp-worker2 xxx.xxx.xxx.xxx xxxxxxxx Note I recommend you use the Public IPs for all your virtual machines. You can use the Private IP for everything except the PROXY node if you really want to, but you must be consistent in all the next steps for everything to work properly.","title":"Create the Virtual Machines"},{"location":"customurl/","text":"Customizing the cluster access URL You can get your own domain name from IBM Cloud. Once it gets created you can add an A record for your ICP environment. Attention Capture the steps to do this including screen shots To use a custom URL for your cluster follow the instructions here","title":"Using a custom URL"},{"location":"customurl/#customizing-the-cluster-access-url","text":"You can get your own domain name from IBM Cloud. Once it gets created you can add an A record for your ICP environment. Attention Capture the steps to do this including screen shots To use a custom URL for your cluster follow the instructions here","title":"Customizing the cluster access URL"},{"location":"installicp/","text":"Install IBM Cloud Private Note The rest of the installation will take place only on the Master node. At this time you should ssh into your master node as root . Unpack the installer Inside of the virtual machine for your master node, create a new directory called /opt/icp311 in a terminal window on your laptop, execute this command: scp ibm-cloud-private-x86_64-3.1.1.tar.gz root@<your-master-node-ip>:/opt/icp311 Note If you have already transferred it the file will be in /tmp so you can use this command instead: mv /tmp/ibm-cloud-private-x86_64-3.1.1.tar.gz /opt/icp311 Expand the tarball (from /opt/icp311 directory) /usr/bin/tar xf ibm-cloud-private-x86_64-3.1.1.tar.gz -O | docker load Note This command may take several minutes to run Create inception (run the docker image from the /opt/icp311 directory) docker run -v $(pwd):/data -e LICENSE=accept ibmcom/icp-inception-amd64:3.1.1-ee cp -r cluster /data After this command runs, a new directory called cluster will be created. Move the image files for your cluster to the /<installation_directory>/cluster/images folder by performing these steps: Create a new folder under the /cluster directory called images . Run this command (from the installation directory): mv ibm-cloud-private-x86_64-3.1.1.tar.gz cluster/images/ Copy the SSH Key to the keys (run this command from the /opt/icp311 directory) cp ~/.ssh/id_rsa ./cluster/ssh_key Note If you get prompted whether you want to overwrite the file, type yes . Edit the hosts file (found in the /opt/icp311/cluster directory). [master] <your-master-ip> [worker] <your-worker1-ip> <your-worker2-ip> [proxy] <your-master-ip> [management] <your-management-ip> [va] <your-va-ip> Warning The default hosts file has the management and va sections commented out. Be sure to remove the # comment markers or your install will fail!! Also, remove the line with the three dots ... in the [worker] section. Edit cluster/config.yaml file for custom settings ## Advanced Settings default_admin_user: admin default_admin_password: <set your admin password here> management_services: vulnerability-advisor: enabled As root run the installer (from the /opt/icp311/cluster directory) docker run --net=host -t -e LICENSE=accept -v \"$(pwd)\":/installer/cluster ibmcom/icp-inception-amd64:3.1.1-ee install Note If all goes well your install will finish successfully and you will be good to go. Warning If the install fails you need to run the uninstall command before you run the installer again. To troubleshoot the issue you can find the logs in the cluster/logs directory. docker run --net=host -t -e LICENSE=accept -v \"$(pwd)\":/installer/cluster ibmcom/icp-inception-amd64:3.1.1-ee uninstall Happy hosting!","title":"Installation"},{"location":"installicp/#install-ibm-cloud-private","text":"Note The rest of the installation will take place only on the Master node. At this time you should ssh into your master node as root .","title":"Install IBM Cloud Private"},{"location":"installicp/#unpack-the-installer","text":"Inside of the virtual machine for your master node, create a new directory called /opt/icp311 in a terminal window on your laptop, execute this command: scp ibm-cloud-private-x86_64-3.1.1.tar.gz root@<your-master-node-ip>:/opt/icp311 Note If you have already transferred it the file will be in /tmp so you can use this command instead: mv /tmp/ibm-cloud-private-x86_64-3.1.1.tar.gz /opt/icp311 Expand the tarball (from /opt/icp311 directory) /usr/bin/tar xf ibm-cloud-private-x86_64-3.1.1.tar.gz -O | docker load Note This command may take several minutes to run Create inception (run the docker image from the /opt/icp311 directory) docker run -v $(pwd):/data -e LICENSE=accept ibmcom/icp-inception-amd64:3.1.1-ee cp -r cluster /data After this command runs, a new directory called cluster will be created. Move the image files for your cluster to the /<installation_directory>/cluster/images folder by performing these steps: Create a new folder under the /cluster directory called images . Run this command (from the installation directory): mv ibm-cloud-private-x86_64-3.1.1.tar.gz cluster/images/ Copy the SSH Key to the keys (run this command from the /opt/icp311 directory) cp ~/.ssh/id_rsa ./cluster/ssh_key Note If you get prompted whether you want to overwrite the file, type yes . Edit the hosts file (found in the /opt/icp311/cluster directory). [master] <your-master-ip> [worker] <your-worker1-ip> <your-worker2-ip> [proxy] <your-master-ip> [management] <your-management-ip> [va] <your-va-ip> Warning The default hosts file has the management and va sections commented out. Be sure to remove the # comment markers or your install will fail!! Also, remove the line with the three dots ... in the [worker] section. Edit cluster/config.yaml file for custom settings ## Advanced Settings default_admin_user: admin default_admin_password: <set your admin password here> management_services: vulnerability-advisor: enabled As root run the installer (from the /opt/icp311/cluster directory) docker run --net=host -t -e LICENSE=accept -v \"$(pwd)\":/installer/cluster ibmcom/icp-inception-amd64:3.1.1-ee install Note If all goes well your install will finish successfully and you will be good to go. Warning If the install fails you need to run the uninstall command before you run the installer again. To troubleshoot the issue you can find the logs in the cluster/logs directory. docker run --net=host -t -e LICENSE=accept -v \"$(pwd)\":/installer/cluster ibmcom/icp-inception-amd64:3.1.1-ee uninstall Happy hosting!","title":"Unpack the installer"},{"location":"ldap/","text":"Installing and Configuring LDAP THANK YOU to Jim Conallen, for the following documentation!! Est. Time: 15 min. Note The original directions for configuring LDAP written my Jim Conallen can be found here . Introduction This document describes how to use setup OpenLDAP on a Linux node that is either part of an ICP cluster, or has visibility to and from the cluster. It uses the Docker image siji/openldap:2.4.42 which is a multi-arch image with an OpenLDAP server, and a web based administrative console. This document describes how to install and setup a few users and groups that can then brought into ICP teams. You can run the LDAP server on any one of the nodes, but I would suggest you run it either on the master or boot node. I choose to run this on a separate stand alone server. All it requires is that docker is installed. Warning If you plan in Customizing the cluster access URL you should configure that BEFORE you configure any LDAP connections. Installation The VM that will run OpenLDAP must have docker installed. If it does not have visibility to DockerHub to directly pull the image siji/openldap:2.4.42, then you will have to save the image to a file and copy it image from another machine that does with the docker image save and docker image load commands. To ensure persistence across reboots you will need two dedicated directories on the VM hosting OpenLDAP, one for /etc/ldap/slapd.d and /var/lib/ldap. Create directories in /var/openldap for these with the commands: mkdir -p /opt/openldap/slapd.d mkdir -p /opt/openldap/ldap With the image in the local registry (or if the machine has visibility to DockerHub), run the following command. You can change DOMAIN to match the cluster name and domain of the ICP cluster that you will connect to, but this is option and only makes sense if this ldap is used with only that one cluster. docker run -d -e DOMAIN=mycluster.icp --net=host --name=openldap \\--restart unless-stopped \\ -v /opt/openldap/ldap:/var/lib/ldap \\ -v /opt/openldap/slapd.d:/etc/ldap/slapd.d \\ siji/openldap:2.4.42 This will run OpenLDAP and with the restart option ensure that it runs after reboots. Note The command above will run a docker image containing LDAP and set it to automatically restart if the virtual machine is restarted. It also assumes you named your ICP cluster mycluster.icp ; if you changed it to something different use that value for the DOMAIN parameter in the command above. Configuring Users and Groups With a web browser log into the phpLDAPAdmin page, the address is: http://[ IP Address of LDAP VM ]:9580/phpldapadmin The default user/password is admin/admin. Users Click on the top level element the tree (dc=mycluster, dc=icp). Create a child entry of type organizational unit called users , and click confirm . Create a child entry of users, of Default type. Select inetOrgPerson , organizationalPerson , and person as object classes. Click Proceed . Then select User Name (uid) for the RDN , and enter in the username for the cn and sn fields. Create one for yourself first, you\u2019ll do this same procedure for all users. Scroll down and set the Password field, and the User Name field with the value of your username. Click Create then click Commit to save the changes. Continue to create as many users as you want. After you have created the users, you need to create at least one group. Groups Create a child entry off the top node of type Organisational Unit , call it groups . Click Create Object and Commit . Create a child entry of groups with a Default type. Select the Object Class groupOfUniqueNames , and click Proceed . Select cn as the RDN type, and enter the group name users . You must add at least one actual user to this group, choose your user id. The uniqueMember must be of the form; uid=<username>, ou=<username>, dc=mycluster, dc=icp Click Create and Commit . Now you can add the remaining users by clicking the modify group members link. Add the remaining users to the group using the UI, then click Save Changes . Configure ICP From the ICP UI, and logged in as the Cluster Administrator, select Manage > Authentication from the main menu, and click on the link to configure authentication. Enter in the following values for the fields: Name: ldap Type: Custom URL: ldap://[IP Address of LDAP VM]:389 Base DN: dc=mycluster, dc=icp Bind DN: cn=admin, dc=mycluster, dc=icp Admin password: admin Warning The value for admin password needs to be whatever you set as the admin password for your ICP Cluster. The default is admin but if you changed it during your installation use your password here. The values of dc fields above assume you used mycluster.icp as the value for DOMAIN when you created the LDAP server. If you used other values, then you need to modify the values for dc in these instructions as well. For example, if you set the DOMAIN to davecluster.dwicp then the Base DN would be dc=davecluster, dc=dwicp and the Bind DN would be cn=admin, dc=davecluster, dc=dwicp . Click the Test Connection button to verify that these are the correct values. Scroll down and edit the User filter so that it equals: (&(uid=%v)(objectclass=person)) . Then click Save . The LDAP server should now be configured with ICP. You can verify by creating a Team . Create a Team From the Manage > Teams menu Create a new team. Enter in a name for the team (e.g. developers), and click on the Users selection. Then start typing the name of a user you created in the search field just below. When you find it check it (to select it) and then select the role for this user in the team. If you can find the users you know that the LDAP was configured properly. That's it!","title":"Installing and Configuring LDAP"},{"location":"ldap/#installing-and-configuring-ldap","text":"THANK YOU to Jim Conallen, for the following documentation!! Est. Time: 15 min. Note The original directions for configuring LDAP written my Jim Conallen can be found here .","title":"Installing and Configuring LDAP"},{"location":"ldap/#introduction","text":"This document describes how to use setup OpenLDAP on a Linux node that is either part of an ICP cluster, or has visibility to and from the cluster. It uses the Docker image siji/openldap:2.4.42 which is a multi-arch image with an OpenLDAP server, and a web based administrative console. This document describes how to install and setup a few users and groups that can then brought into ICP teams. You can run the LDAP server on any one of the nodes, but I would suggest you run it either on the master or boot node. I choose to run this on a separate stand alone server. All it requires is that docker is installed. Warning If you plan in Customizing the cluster access URL you should configure that BEFORE you configure any LDAP connections.","title":"Introduction"},{"location":"ldap/#installation","text":"The VM that will run OpenLDAP must have docker installed. If it does not have visibility to DockerHub to directly pull the image siji/openldap:2.4.42, then you will have to save the image to a file and copy it image from another machine that does with the docker image save and docker image load commands. To ensure persistence across reboots you will need two dedicated directories on the VM hosting OpenLDAP, one for /etc/ldap/slapd.d and /var/lib/ldap. Create directories in /var/openldap for these with the commands: mkdir -p /opt/openldap/slapd.d mkdir -p /opt/openldap/ldap With the image in the local registry (or if the machine has visibility to DockerHub), run the following command. You can change DOMAIN to match the cluster name and domain of the ICP cluster that you will connect to, but this is option and only makes sense if this ldap is used with only that one cluster. docker run -d -e DOMAIN=mycluster.icp --net=host --name=openldap \\--restart unless-stopped \\ -v /opt/openldap/ldap:/var/lib/ldap \\ -v /opt/openldap/slapd.d:/etc/ldap/slapd.d \\ siji/openldap:2.4.42 This will run OpenLDAP and with the restart option ensure that it runs after reboots. Note The command above will run a docker image containing LDAP and set it to automatically restart if the virtual machine is restarted. It also assumes you named your ICP cluster mycluster.icp ; if you changed it to something different use that value for the DOMAIN parameter in the command above.","title":"Installation"},{"location":"ldap/#configuring-users-and-groups","text":"With a web browser log into the phpLDAPAdmin page, the address is: http://[ IP Address of LDAP VM ]:9580/phpldapadmin The default user/password is admin/admin.","title":"Configuring Users and Groups"},{"location":"ldap/#users","text":"Click on the top level element the tree (dc=mycluster, dc=icp). Create a child entry of type organizational unit called users , and click confirm . Create a child entry of users, of Default type. Select inetOrgPerson , organizationalPerson , and person as object classes. Click Proceed . Then select User Name (uid) for the RDN , and enter in the username for the cn and sn fields. Create one for yourself first, you\u2019ll do this same procedure for all users. Scroll down and set the Password field, and the User Name field with the value of your username. Click Create then click Commit to save the changes. Continue to create as many users as you want. After you have created the users, you need to create at least one group.","title":"Users"},{"location":"ldap/#groups","text":"Create a child entry off the top node of type Organisational Unit , call it groups . Click Create Object and Commit . Create a child entry of groups with a Default type. Select the Object Class groupOfUniqueNames , and click Proceed . Select cn as the RDN type, and enter the group name users . You must add at least one actual user to this group, choose your user id. The uniqueMember must be of the form; uid=<username>, ou=<username>, dc=mycluster, dc=icp Click Create and Commit . Now you can add the remaining users by clicking the modify group members link. Add the remaining users to the group using the UI, then click Save Changes .","title":"Groups"},{"location":"ldap/#configure-icp","text":"From the ICP UI, and logged in as the Cluster Administrator, select Manage > Authentication from the main menu, and click on the link to configure authentication. Enter in the following values for the fields: Name: ldap Type: Custom URL: ldap://[IP Address of LDAP VM]:389 Base DN: dc=mycluster, dc=icp Bind DN: cn=admin, dc=mycluster, dc=icp Admin password: admin Warning The value for admin password needs to be whatever you set as the admin password for your ICP Cluster. The default is admin but if you changed it during your installation use your password here. The values of dc fields above assume you used mycluster.icp as the value for DOMAIN when you created the LDAP server. If you used other values, then you need to modify the values for dc in these instructions as well. For example, if you set the DOMAIN to davecluster.dwicp then the Base DN would be dc=davecluster, dc=dwicp and the Bind DN would be cn=admin, dc=davecluster, dc=dwicp . Click the Test Connection button to verify that these are the correct values. Scroll down and edit the User filter so that it equals: (&(uid=%v)(objectclass=person)) . Then click Save . The LDAP server should now be configured with ICP. You can verify by creating a Team .","title":"Configure ICP"},{"location":"ldap/#create-a-team","text":"From the Manage > Teams menu Create a new team. Enter in a name for the team (e.g. developers), and click on the Users selection. Then start typing the name of a user you created in the search field just below. When you find it check it (to select it) and then select the role for this user in the team. If you can find the users you know that the LDAP was configured properly. That's it!","title":"Create a Team"},{"location":"mcm/","text":"Download the tarball You will need the tarball that contains the MCM images. Depending on your ICP architecture, here are the files you need. mcm-3.1.1-amd64.tgz (for x86) mcm-3.1.1-ppc64le.tgz (for power) Both of these files can be found in the mcm-3.1.1.tgz tarball file, which is the Installation Package Assembly. Note If you are an IBM'er, you can download the install files from the Software Sellers Workplace . You will need to log in and do a search for IBM Multi-cloud Manager 3.1.1 . The package assembly you need to look for is: IBM Multi-cloud Manager Installation Packages eAssembly (CJ4NTEN) Download the following part in the assembly: IBM Multi-cloud Manager 3.1.1 Kubernetes Images for Linux (CNX22EN) For the rest of these instructions, we are assuming an x86 architecture. The following instructions come basically from the Knowledge Center for installing MCM https://www.ibm.com/support/knowledgecenter/en/SSBS6K_3.1.1/mcm/installing/install.html Prerequisites The following must be installed on your MacBook as prerequisites for these instructions Access to your ICP instance Docker kubectl helm IBM Cloud Private CLI (cloudctl) Loading the archive The first step is to get the MCM images into the ICP Docker registry. You need to remotely login to the ICP Docker registry from your laptop. For these instructions a MacBook was used. See the Knowledge Center link above for instructions for Linux. In order to get access to the remote ICP Docker registry, you need to get an authorization certificate. The certificate authority is located on your master node and is by default named mycluster.icp . If you gave it another name when installing then use that name. All of the following instructions assume the certificate authority name is mycluster.icp . Execute these steps be able to login to your ICP Docker registry. Not sure if all of these are necessary. First, add your certificate authority (CA) name to your local /etc/hosts file. <icp-master-ip> mycluster.icp Then, on your MacBook create a new directory to store the certificate and copy the cert file down from your CA node mkdir -p ~/.docker/certs.d/mycluster.icp\\:8500 scp root@mycluster.icp:/etc/docker/certs.d/mycluster.icp\\:8500/ca.crt ~/.docker/certs.d/mycluster.icp\\:8500/ca.crt Also, add the cert to your MacBook keychain sudo security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain ~/.docker/certs.d/mycluster.icp\\:8500/ca.crt Add the remote certificate authority as an insecure Docker registry. From the Docker application on the MacBook, select Docker->Preferences. On the Deamon tab, add mycluster.icp:8500 as an insecure registry. It should look like this. Apply and restart Docker After all of these steps, you should be able to login to your ICP Docker registry with this command. docker login mycluster.icp:8500 Now login to cloudctl cloudctl login -a https://<master-cluster-ip>:8443 --skip-ssl-validation From the directory that contains the mcm-3.1.1-amd64.tgz (extracted from the mcm-3.1.1.tgz file), load the archives using the command below. You will see a series of images being loaded into the registry. When done, via your ICP management console, verify that images starting with mcm are now loaded to Container Images . cloudctl catalog load-ppa-archive -a mcm-3.1.1-amd64.tgz --registry mycluster.icp:8500/kube-system You should also see 2 new MCM entries in the catalog Install Multi-cloud Manager Installing Multi-cloud Manager has many options. Below we will be basically taking the default route. During the installation process it also lets you install the Multi-cloud Manager klusterlet in the same step. We will do that separately. Login to your ICP Management Console Via the catalog, select the ibm-mcm-prod (Multi-cloud Manager) Helm chart Click configure Specify the following settings For the Helm Release Name, specify any name Select kube-system for the Target namespace (very important) Check the License checkbox The Pod Security Box should already be populated. Don't change it. Open the Quick Start parameters twisty Specify mcm as the Multicloud Manager namespace Click Install Once installed, you should see the helm release in the Helm Releases list. You should also see Multicloud Manager as a top level entry in the hamberger menu You must also create the mcm namespace that we specified above. cloudctl login -a https://<hub_cluster_host_name>:8443 --skip-ssl-validation kubectl create namespace mcm Installing the Multi-cloud Klusterlet in your ICP instance We need to gather some information to install the Klusterlet (all installations of the MCM Klusterlet require this info). Login to the ICP Management console Select the user icon, then click Configure client . The following kubectl CLI commands are given and they follow this format: kubectl config set-cluster {cluster_name} --server={hub_cluster_url} --insecure-skip-tls-verify=true kubectl config set-context {cluster_name}-context --cluster={cluster_name} kubectl config set-credentials {cluster_name}-user --token={hub_cluster_token} kubectl config set-context {cluster_name}-context --user={cluster_name}-user --namespace=default kubectl config use-context {cluster_name}-context There are two pieces of information we need from above. The first is the hub_cluster_url (begins with https and ends in port :8001) and the second is the hub_cluster_token (long string). Save these for later steps. Login to your ICP Management Console Via the catalog, select the ibm-mcmk-prod (Multi-cloud Manager Klusterlet) Helm chart Click configure Specify the following settings: For the Helm Release Name, specify any name Select kube-system for the Target namespace (very important) Check the License checkbox The Pod Security Box should already be populated. Don't change it. Open the Quick Start parameters twisty Cluster name: specify a name for the cluster as seen by MCM (example: icp-cluster or dev-cluster ) Cluster namespace: give a unique namespace that MCM will use on the hub cluster for cluster-specific resources (example: mcm-icp or mcm-dev ) Hub Cluster Kubernetes API server: enter the hub_cluster_url obtained above Hub Cluster Kubernetes API server token: enter the hub_cluster_token obtained above (copy carefully!!) Open the All Paramenters twisty Scroll down and check the Enable Automatic Generate Tiller Secret checkbox Scroll down and check the Deploy to Hub Cluster? checkbox The following fields are simply labels assigned to the cluster and will be used when matching resources to clusters. Enter what you would like. Cluster Cloud Provider Kubernetes Vendor Cluster Environment Type Cluster Region Cluster Datacenter Cluster Owner Click Local Install Once installed, you should see the helm release in the Helm Releases list. Also, in the ICP Management Console, click the Multicloud Manager entry in the left hamberger menu. Once in the Multicloud Manager, select Clusters in the left hamberger menu. You should see your cluster present. Notice the labels that we specified during installation. Download the MCM CLI. The first command downloads the executable mcmctl into your current directory. The second one moves it to a directory in your path, like /usr/local/bin . docker run -e LICENSE=accept -v $(pwd):/data mycluster.icp:8500/kube-system/mcmctl:3.1.1 cp mcmctl-darwin-amd64 /data/mcmctl mv mcmctl /usr/local/bin Install the Multi-cloud Klusterlet into an instance of IKS Installing the MCM Klusterlet into Kubernetes clusters other than ICP will be officially supported in a future release (i.e. 3.1.2), but the following will work. These instructions are obtained from https://hub.docker.com/r/ibmcom/mcm-inception-amd64 . The instructions install the CE version of the Klusterlet. Run the following command to get the cluster directory from the inception container. The result of this command is that a cluster directory is created in your current directory. docker run -v $(pwd):/data -e LICENSE=accept ibmcom/mcm-inception-amd64:3.1.1-ce cp -r cluster /data Obtain the kubeconfig zip file from your IKS cluster. Follow these steps: Log in to the IBM Cloud console (cloud.ibm.com) where you have installed IKS From your list of resources, select your IKS cluster Once on the cluster overview page, select the Access tab Down near the bottom of the Access tab is a download link. Click the link to download the kubeconfig file (it is a zip file but does not have an extension) In the cluster directory created in the first step, copy the newly downloaded kubeconfig file ontop of the existing kubeconfig file. change directories to the cluster directory (if you haven't already) Edit the config.yaml file to add values to the following properties: cluster-name: the name of the IKS cluster as it will be known by MCM (example: iks-cluster ) cluster-namespace: the namespace that will be created on the hub cluster for resources specific to the IKS cluster (example: mcm-iks ) cluster-tags: a list of tags that will be used when assigning resources to this cluster. These are the same tags that we specified when installing the ICP Klusterlet. See the example file below. prometheus-ingress-port: leave as 9090 hub-k8s-endpoint: the hub_cluster_url we used when installing the ICP Klusterlet hub-k8s-token: the hub_cluster_token we used when installing the ICP Klusterlet Example config.yaml file Run the inception container to install the Klusterlet with the following command. docker run --net=host -t -e LICENSE=accept -v \"$(pwd)\":/installer/cluster ibmcom/mcm-inception-amd64:3.1.1-ce install-mcmk-on-iks -v Back to your MCM console, you should now see the IKS cluster in your clusters list Congratulations. You should now have Multi-cloud Manager installed managing your ICP and IKS clusters!!!","title":"installing Muticloud Manager"},{"location":"mcm/#download-the-tarball","text":"You will need the tarball that contains the MCM images. Depending on your ICP architecture, here are the files you need. mcm-3.1.1-amd64.tgz (for x86) mcm-3.1.1-ppc64le.tgz (for power) Both of these files can be found in the mcm-3.1.1.tgz tarball file, which is the Installation Package Assembly. Note If you are an IBM'er, you can download the install files from the Software Sellers Workplace . You will need to log in and do a search for IBM Multi-cloud Manager 3.1.1 . The package assembly you need to look for is: IBM Multi-cloud Manager Installation Packages eAssembly (CJ4NTEN) Download the following part in the assembly: IBM Multi-cloud Manager 3.1.1 Kubernetes Images for Linux (CNX22EN) For the rest of these instructions, we are assuming an x86 architecture. The following instructions come basically from the Knowledge Center for installing MCM https://www.ibm.com/support/knowledgecenter/en/SSBS6K_3.1.1/mcm/installing/install.html","title":"Download the tarball"},{"location":"mcm/#prerequisites","text":"The following must be installed on your MacBook as prerequisites for these instructions Access to your ICP instance Docker kubectl helm IBM Cloud Private CLI (cloudctl)","title":"Prerequisites"},{"location":"mcm/#loading-the-archive","text":"The first step is to get the MCM images into the ICP Docker registry. You need to remotely login to the ICP Docker registry from your laptop. For these instructions a MacBook was used. See the Knowledge Center link above for instructions for Linux. In order to get access to the remote ICP Docker registry, you need to get an authorization certificate. The certificate authority is located on your master node and is by default named mycluster.icp . If you gave it another name when installing then use that name. All of the following instructions assume the certificate authority name is mycluster.icp . Execute these steps be able to login to your ICP Docker registry. Not sure if all of these are necessary. First, add your certificate authority (CA) name to your local /etc/hosts file. <icp-master-ip> mycluster.icp Then, on your MacBook create a new directory to store the certificate and copy the cert file down from your CA node mkdir -p ~/.docker/certs.d/mycluster.icp\\:8500 scp root@mycluster.icp:/etc/docker/certs.d/mycluster.icp\\:8500/ca.crt ~/.docker/certs.d/mycluster.icp\\:8500/ca.crt Also, add the cert to your MacBook keychain sudo security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain ~/.docker/certs.d/mycluster.icp\\:8500/ca.crt Add the remote certificate authority as an insecure Docker registry. From the Docker application on the MacBook, select Docker->Preferences. On the Deamon tab, add mycluster.icp:8500 as an insecure registry. It should look like this. Apply and restart Docker After all of these steps, you should be able to login to your ICP Docker registry with this command. docker login mycluster.icp:8500 Now login to cloudctl cloudctl login -a https://<master-cluster-ip>:8443 --skip-ssl-validation From the directory that contains the mcm-3.1.1-amd64.tgz (extracted from the mcm-3.1.1.tgz file), load the archives using the command below. You will see a series of images being loaded into the registry. When done, via your ICP management console, verify that images starting with mcm are now loaded to Container Images . cloudctl catalog load-ppa-archive -a mcm-3.1.1-amd64.tgz --registry mycluster.icp:8500/kube-system You should also see 2 new MCM entries in the catalog","title":"Loading the archive"},{"location":"mcm/#install-multi-cloud-manager","text":"Installing Multi-cloud Manager has many options. Below we will be basically taking the default route. During the installation process it also lets you install the Multi-cloud Manager klusterlet in the same step. We will do that separately. Login to your ICP Management Console Via the catalog, select the ibm-mcm-prod (Multi-cloud Manager) Helm chart Click configure Specify the following settings For the Helm Release Name, specify any name Select kube-system for the Target namespace (very important) Check the License checkbox The Pod Security Box should already be populated. Don't change it. Open the Quick Start parameters twisty Specify mcm as the Multicloud Manager namespace Click Install Once installed, you should see the helm release in the Helm Releases list. You should also see Multicloud Manager as a top level entry in the hamberger menu You must also create the mcm namespace that we specified above. cloudctl login -a https://<hub_cluster_host_name>:8443 --skip-ssl-validation kubectl create namespace mcm","title":"Install Multi-cloud Manager"},{"location":"mcm/#installing-the-multi-cloud-klusterlet-in-your-icp-instance","text":"We need to gather some information to install the Klusterlet (all installations of the MCM Klusterlet require this info). Login to the ICP Management console Select the user icon, then click Configure client . The following kubectl CLI commands are given and they follow this format: kubectl config set-cluster {cluster_name} --server={hub_cluster_url} --insecure-skip-tls-verify=true kubectl config set-context {cluster_name}-context --cluster={cluster_name} kubectl config set-credentials {cluster_name}-user --token={hub_cluster_token} kubectl config set-context {cluster_name}-context --user={cluster_name}-user --namespace=default kubectl config use-context {cluster_name}-context There are two pieces of information we need from above. The first is the hub_cluster_url (begins with https and ends in port :8001) and the second is the hub_cluster_token (long string). Save these for later steps. Login to your ICP Management Console Via the catalog, select the ibm-mcmk-prod (Multi-cloud Manager Klusterlet) Helm chart Click configure Specify the following settings: For the Helm Release Name, specify any name Select kube-system for the Target namespace (very important) Check the License checkbox The Pod Security Box should already be populated. Don't change it. Open the Quick Start parameters twisty Cluster name: specify a name for the cluster as seen by MCM (example: icp-cluster or dev-cluster ) Cluster namespace: give a unique namespace that MCM will use on the hub cluster for cluster-specific resources (example: mcm-icp or mcm-dev ) Hub Cluster Kubernetes API server: enter the hub_cluster_url obtained above Hub Cluster Kubernetes API server token: enter the hub_cluster_token obtained above (copy carefully!!) Open the All Paramenters twisty Scroll down and check the Enable Automatic Generate Tiller Secret checkbox Scroll down and check the Deploy to Hub Cluster? checkbox The following fields are simply labels assigned to the cluster and will be used when matching resources to clusters. Enter what you would like. Cluster Cloud Provider Kubernetes Vendor Cluster Environment Type Cluster Region Cluster Datacenter Cluster Owner Click Local Install Once installed, you should see the helm release in the Helm Releases list. Also, in the ICP Management Console, click the Multicloud Manager entry in the left hamberger menu. Once in the Multicloud Manager, select Clusters in the left hamberger menu. You should see your cluster present. Notice the labels that we specified during installation. Download the MCM CLI. The first command downloads the executable mcmctl into your current directory. The second one moves it to a directory in your path, like /usr/local/bin . docker run -e LICENSE=accept -v $(pwd):/data mycluster.icp:8500/kube-system/mcmctl:3.1.1 cp mcmctl-darwin-amd64 /data/mcmctl mv mcmctl /usr/local/bin","title":"Installing the Multi-cloud Klusterlet in your ICP instance"},{"location":"mcm/#install-the-multi-cloud-klusterlet-into-an-instance-of-iks","text":"Installing the MCM Klusterlet into Kubernetes clusters other than ICP will be officially supported in a future release (i.e. 3.1.2), but the following will work. These instructions are obtained from https://hub.docker.com/r/ibmcom/mcm-inception-amd64 . The instructions install the CE version of the Klusterlet. Run the following command to get the cluster directory from the inception container. The result of this command is that a cluster directory is created in your current directory. docker run -v $(pwd):/data -e LICENSE=accept ibmcom/mcm-inception-amd64:3.1.1-ce cp -r cluster /data Obtain the kubeconfig zip file from your IKS cluster. Follow these steps: Log in to the IBM Cloud console (cloud.ibm.com) where you have installed IKS From your list of resources, select your IKS cluster Once on the cluster overview page, select the Access tab Down near the bottom of the Access tab is a download link. Click the link to download the kubeconfig file (it is a zip file but does not have an extension) In the cluster directory created in the first step, copy the newly downloaded kubeconfig file ontop of the existing kubeconfig file. change directories to the cluster directory (if you haven't already) Edit the config.yaml file to add values to the following properties: cluster-name: the name of the IKS cluster as it will be known by MCM (example: iks-cluster ) cluster-namespace: the namespace that will be created on the hub cluster for resources specific to the IKS cluster (example: mcm-iks ) cluster-tags: a list of tags that will be used when assigning resources to this cluster. These are the same tags that we specified when installing the ICP Klusterlet. See the example file below. prometheus-ingress-port: leave as 9090 hub-k8s-endpoint: the hub_cluster_url we used when installing the ICP Klusterlet hub-k8s-token: the hub_cluster_token we used when installing the ICP Klusterlet Example config.yaml file Run the inception container to install the Klusterlet with the following command. docker run --net=host -t -e LICENSE=accept -v \"$(pwd)\":/installer/cluster ibmcom/mcm-inception-amd64:3.1.1-ce install-mcmk-on-iks -v Back to your MCM console, you should now see the IKS cluster in your clusters list Congratulations. You should now have Multi-cloud Manager installed managing your ICP and IKS clusters!!!","title":"Install the Multi-cloud Klusterlet into an instance of IKS"},{"location":"setupall/","text":"Setup on All Machines To install IBM Cloud Private you first need to prepare all of your virtual machines. These instructions are specific to the install being done, and were derived from the Preparing your cluster for installation section of the ICP Knowledge Center. They are not reflective of all the steps necessary in all situations. Important There are two scripts that will help you prapare your virtual machines for installation of ICP. modify_fs_v2.sh will prepare the disks and move the /var and /opt directories to the two additional disks provisioned when the VMs were created. install_prereqs.sh takes care of installing MOST of what you will need for the prerequisites. I would encourage you to look at the contents of these scripts to understand what they do. It should also be noted that they are specific to RHEL, so if your using another OS, such as Ubuntu you will have to perform these steps manually using the proper os commands. New We have added an additional script to the repo that will expand the /var directory only. We have found that the space requirements for v3.1.1 have changed from previous the version. For this reason, we have a 3rd script you can choose to use if you only want to increase the size of the /var directory. modify_fs_v3.sh will prepare the disk and move the /var directorie to the additional disk provisioned when the VMs were created. Install Prerequisites Note The first time you make a secure connection to a remote machine you may see this message: The authenticity of host '169.60.185.59 (169.60.185.59)' can't be established. ECDSA key fingerprint is SHA256:XAs372uCWTkOqLOkXwQYuCXq21GaJFoYIuItUf0xGpc. Are you sure you want to continue connecting (yes/no)? Type in yes and the key fingerprint will be added to your ~/.ssh/known_hosts file. Download install_prereqs.sh and use scp to transfer it to the virtual machine. Inside of the virtual machine use this command to change the permissions: chmod 777 install_prereqs.sh Execute the script: ./install_prereqs.sh Modify the disks When the virtual machine was created two additional disks were added to the configuration, but they are not yet configured inside the virtual machine. These two disks are to be used for the /opt and /var directories. Download modify_fs_v2.sh and use scp to transfer it to the virtual machine. Inside of the virtual machine use this command to change permissions: chmod 777 modify_fs_v2.sh Execute the script: ./modify_fs_v2.sh 199 499 <--- The parameters correspond to the sizes of Disk 1 and Disk 2 (minus 1) Install Docker SCP copy the docker file (This is the IBM Docker Enterprise version) to your local machine (icp-docker-18.03.1_x86_64.bin is available with the download of ICP 3.1.1 EE) From your local machine you need to SCP transfer the file to each ICP server Make an install dir on server (In this example I created the install dir on the root dir) mkdir install chmod dir 777 install Open Terminal window on local machine Navigate to dir where file is located scp command: scp ./icp-docker-18.03.1_x86_64.bin login@icp-server-ip:/install/ On the server you should now see the docker file. Docker install commands chmod 777 on the file ./icp-docker-18.03.1_x86_64.bin --install systemctl start docker systemctl enable docker Disable the firewall systemctl disable firewalld systemctl stop firewalld Rename the virtual machine hostnamectl set-hostname <new name> Update the hosts file Make sure that the /etc/hosts file on each virtual machine has entries for all of the other virtual machines so that they can talk to each other. Take note of what your host names are, you will need them in the following steps. Setup SSH keys Generate the KEY on each node cd /root ;ssh-keygen -b 4096 -f /root/.ssh/id_rsa -N \"\" ;cat ~/.ssh/id_rsa.pub | sudo tee -a ~/.ssh/authorized_keys Copy the key to each of the other nodes DO NOT just copy these commands below, they are for example ONLY. You need to edit the host names at the end of each of these commands. You will need to run ALL of these commands to copy the SSH key from the current VM to all the rest of the VM's in the cluster. We are sharing the SSH keys, so that the install script can SSH into the other boxes without authentication. ssh-copy-id -i .ssh/id_rsa.pub root@dak311master ssh-copy-id -i .ssh/id_rsa.pub root@dak311mgmt ssh-copy-id -i .ssh/id_rsa.pub root@dak311va ssh-copy-id -i .ssh/id_rsa.pub root@dak311worker1 ssh-copy-id -i .ssh/id_rsa.pub root@dak311worker2 Test the SSH keys You can perform a simple test to ensure that the SSH keys are setup correctly. At this point you should be able to SSH into any one of the VM's from any one of the VM's. Try it out and spot check the SSH keys. As an example: I am logged into the master node. I should be able to type this command below and log directly into the worker1 node without any authentication propts. The example below assumes the host name for worker1 is dak311worker1, yours may be different. ssh dak311worker1 Note You should now see that your logged into the worker 1 node and you didn't see any prompts for userid or password.","title":"Setup on All VMs"},{"location":"setupall/#setup-on-all-machines","text":"To install IBM Cloud Private you first need to prepare all of your virtual machines. These instructions are specific to the install being done, and were derived from the Preparing your cluster for installation section of the ICP Knowledge Center. They are not reflective of all the steps necessary in all situations. Important There are two scripts that will help you prapare your virtual machines for installation of ICP. modify_fs_v2.sh will prepare the disks and move the /var and /opt directories to the two additional disks provisioned when the VMs were created. install_prereqs.sh takes care of installing MOST of what you will need for the prerequisites. I would encourage you to look at the contents of these scripts to understand what they do. It should also be noted that they are specific to RHEL, so if your using another OS, such as Ubuntu you will have to perform these steps manually using the proper os commands. New We have added an additional script to the repo that will expand the /var directory only. We have found that the space requirements for v3.1.1 have changed from previous the version. For this reason, we have a 3rd script you can choose to use if you only want to increase the size of the /var directory. modify_fs_v3.sh will prepare the disk and move the /var directorie to the additional disk provisioned when the VMs were created.","title":"Setup on All Machines"},{"location":"setupall/#install-prerequisites","text":"Note The first time you make a secure connection to a remote machine you may see this message: The authenticity of host '169.60.185.59 (169.60.185.59)' can't be established. ECDSA key fingerprint is SHA256:XAs372uCWTkOqLOkXwQYuCXq21GaJFoYIuItUf0xGpc. Are you sure you want to continue connecting (yes/no)? Type in yes and the key fingerprint will be added to your ~/.ssh/known_hosts file. Download install_prereqs.sh and use scp to transfer it to the virtual machine. Inside of the virtual machine use this command to change the permissions: chmod 777 install_prereqs.sh Execute the script: ./install_prereqs.sh","title":"Install Prerequisites"},{"location":"setupall/#modify-the-disks","text":"When the virtual machine was created two additional disks were added to the configuration, but they are not yet configured inside the virtual machine. These two disks are to be used for the /opt and /var directories. Download modify_fs_v2.sh and use scp to transfer it to the virtual machine. Inside of the virtual machine use this command to change permissions: chmod 777 modify_fs_v2.sh Execute the script: ./modify_fs_v2.sh 199 499 <--- The parameters correspond to the sizes of Disk 1 and Disk 2 (minus 1)","title":"Modify the disks"},{"location":"setupall/#install-docker","text":"SCP copy the docker file (This is the IBM Docker Enterprise version) to your local machine (icp-docker-18.03.1_x86_64.bin is available with the download of ICP 3.1.1 EE) From your local machine you need to SCP transfer the file to each ICP server Make an install dir on server (In this example I created the install dir on the root dir) mkdir install chmod dir 777 install Open Terminal window on local machine Navigate to dir where file is located scp command: scp ./icp-docker-18.03.1_x86_64.bin login@icp-server-ip:/install/ On the server you should now see the docker file.","title":"Install Docker"},{"location":"setupall/#docker-install-commands","text":"chmod 777 on the file ./icp-docker-18.03.1_x86_64.bin --install systemctl start docker systemctl enable docker","title":"Docker install commands"},{"location":"setupall/#disable-the-firewall","text":"systemctl disable firewalld systemctl stop firewalld","title":"Disable the firewall"},{"location":"setupall/#rename-the-virtual-machine","text":"hostnamectl set-hostname <new name>","title":"Rename the virtual machine"},{"location":"setupall/#update-the-hosts-file","text":"Make sure that the /etc/hosts file on each virtual machine has entries for all of the other virtual machines so that they can talk to each other. Take note of what your host names are, you will need them in the following steps.","title":"Update the hosts file"},{"location":"setupall/#setup-ssh-keys","text":"","title":"Setup SSH keys"},{"location":"setupall/#generate-the-key-on-each-node","text":"cd /root ;ssh-keygen -b 4096 -f /root/.ssh/id_rsa -N \"\" ;cat ~/.ssh/id_rsa.pub | sudo tee -a ~/.ssh/authorized_keys","title":"Generate the KEY on each node"},{"location":"setupall/#copy-the-key-to-each-of-the-other-nodes","text":"DO NOT just copy these commands below, they are for example ONLY. You need to edit the host names at the end of each of these commands. You will need to run ALL of these commands to copy the SSH key from the current VM to all the rest of the VM's in the cluster. We are sharing the SSH keys, so that the install script can SSH into the other boxes without authentication. ssh-copy-id -i .ssh/id_rsa.pub root@dak311master ssh-copy-id -i .ssh/id_rsa.pub root@dak311mgmt ssh-copy-id -i .ssh/id_rsa.pub root@dak311va ssh-copy-id -i .ssh/id_rsa.pub root@dak311worker1 ssh-copy-id -i .ssh/id_rsa.pub root@dak311worker2","title":"Copy the key to each of the other nodes"},{"location":"setupall/#test-the-ssh-keys","text":"You can perform a simple test to ensure that the SSH keys are setup correctly. At this point you should be able to SSH into any one of the VM's from any one of the VM's. Try it out and spot check the SSH keys. As an example: I am logged into the master node. I should be able to type this command below and log directly into the worker1 node without any authentication propts. The example below assumes the host name for worker1 is dak311worker1, yours may be different. ssh dak311worker1 Note You should now see that your logged into the worker 1 node and you didn't see any prompts for userid or password.","title":"Test the SSH keys"}]}